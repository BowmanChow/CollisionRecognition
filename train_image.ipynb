{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n['toothpaste_box', 'whiteboard_spray', 'toy_elephant', 'green_basketball', '061_foam_brick', 'shiny_toy_gun', 'salt_cylinder', 'strawberry', 'stanley_screwdriver', 'yellow_block']\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import matplotlib.image as pli\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "from PIL import ImageEnhance\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import glob\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import scipy.signal as ss\n",
    "from enum import Enum\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(torch.cuda.is_available())\n",
    "path = './dataset/train'\n",
    "labels = os.listdir(path)\n",
    "pos_train_folders = {l: glob.glob(f'{path}/{l}/[0-9][0-9]*/') for l in labels}\n",
    "pos_val_folders = {l: glob.glob(f'{path}/{l}/[0-9]/') for l in labels}\n",
    "# print(pos_train_folders)\n",
    "# print(pos_val_folders)\n",
    "print(labels)\n",
    "\n",
    "is_plot = False\n",
    "\n",
    "freq_length = 57\n",
    "time_length = 221\n",
    "trainingset_size = 10000\n",
    "val_set_size = 100\n",
    "batch_size = 64 if torch.cuda.is_available() else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from findContourCenter import findContourCenter\n",
    "def crop_img(folder, is_debug):\n",
    "    sp = 100\n",
    "    mask_img_files = glob.glob(f'{folder}mask/*.png')\n",
    "    mask_file = random.choice(mask_img_files)\n",
    "    rgb_file = mask_file.replace('mask', 'rgb').replace('png', 'jpg')\n",
    "    rgb_img = plt.imread(rgb_file)\n",
    "    rgb_img = np.moveaxis(rgb_img, -1, 0)\n",
    "    mask_img = np.zeros((rgb_img.shape[1],rgb_img.shape[2]))\n",
    "    mask_img[20:460, 100:540] = plt.imread(mask_file)\n",
    "    rgb_img = rgb_img * np.uint8(mask_img)\n",
    "    center_mask, _ = findContourCenter(mask_img)\n",
    "    # center_rgb = np.array([center_mask[0] + 20, center_mask[1] + 100])\n",
    "\n",
    "    crop_img = rgb_img[:, max(int(center_mask[0]) - sp, 0): int(center_mask[0]) + sp, max(int(center_mask[1]) - sp, 0): int(center_mask[1]) + sp]\n",
    "    if is_debug:\n",
    "        plt.imshow(mask_img)\n",
    "        plt.show()\n",
    "        plt.imshow(np.moveaxis(rgb_img, 0, -1))\n",
    "        plt.show()\n",
    "        plt.imshow(np.moveaxis(crop_img, 0, -1))\n",
    "        plt.show()\n",
    "\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSet(data.Dataset):\n",
    "    def __init__(self, behav):\n",
    "        if behav == 'train':\n",
    "            self.length = trainingset_size\n",
    "        elif behav == 'val':\n",
    "            self.length = val_set_size\n",
    "        else:\n",
    "            raise Exception('Error')\n",
    "        self.behav = behav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "\n",
    "        label = random.randint(0,9)\n",
    "        if self.behav == 'train':\n",
    "            folder = random.choice(pos_train_folders[labels[label]])\n",
    "        elif self.behav == 'val':\n",
    "            folder = random.choice(pos_val_folders[labels[label]])\n",
    "        else:\n",
    "            raise Exception('Error')\n",
    "        rgb_img = crop_img(folder, is_plot)\n",
    "        img = np.zeros((3,200,200), dtype='uint8')\n",
    "        row_begin = 100 - int(rgb_img.shape[1]/2)\n",
    "        col_begin = 100 - int(rgb_img.shape[2]/2)\n",
    "        img[:, row_begin:row_begin+rgb_img.shape[1], col_begin:col_begin+rgb_img.shape[2]] = rgb_img\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        if is_plot:\n",
    "            print(img.shape)\n",
    "            plt.imshow(np.moveaxis(img.numpy(), 0, -1))\n",
    "            plt.show()\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "train_loader = data.DataLoader(ImageSet('train'), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ImageCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # 200 200\n",
    "            nn.Conv2d(in_channels=3, out_channels=32,\n",
    "                      kernel_size=7),\n",
    "            # 194 194\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.layer1[0].out_channels, out_channels=32,\n",
    "                      kernel_size=6, stride=2),\n",
    "            # 95 95\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.layer2[0].out_channels,\n",
    "                      out_channels=64, kernel_size=6),\n",
    "            # 90 90\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.layer3[0].out_channels,\n",
    "                      out_channels=64, kernel_size=6, stride=2),\n",
    "            # 43 43\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.layer4[0].out_channels,\n",
    "                      out_channels=128, kernel_size=5),\n",
    "            # 39 39\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.layer5[0].out_channels,\n",
    "                      out_channels=128, kernel_size=3, stride=2),\n",
    "            # 19 19\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(self.layer6[0].out_channels, len(labels))\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.layer1(input)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        # print(out.shape)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgNet = ImageCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "state_dict = torch.load('./imgNet.model')\n",
    "imgNet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i = 0,  loss = 0.07257258147001266,\n",
      "        labels = tensor([3, 6, 2, 3, 6, 7, 8, 3, 5, 9, 0, 8, 2, 4, 2, 6, 6, 6, 3, 2, 9, 8, 8, 9,\n",
      "        4, 4, 2, 6, 4, 8, 3, 0, 2, 7, 9, 0, 5, 3, 0, 8, 2, 0, 5, 3, 5, 9, 0, 0,\n",
      "        7, 9, 9, 5, 0, 0, 8, 7, 8, 0, 9, 5, 4, 0, 6, 5], device='cuda:0')\n",
      "        predict = tensor([3, 6, 2, 3, 6, 7, 8, 3, 5, 9, 0, 8, 2, 4, 1, 6, 6, 6, 3, 2, 9, 8, 8, 9,\n",
      "        4, 4, 1, 6, 4, 8, 3, 0, 2, 7, 9, 0, 5, 3, 0, 8, 2, 0, 5, 3, 5, 9, 0, 0,\n",
      "        7, 9, 9, 5, 0, 0, 8, 7, 8, 0, 9, 5, 4, 0, 6, 5], device='cuda:0')\n",
      "        accuracy = 0.96875\n",
      "i = 2,  loss = 0.04665343090891838,\n",
      "        labels = tensor([3, 6, 8, 6, 5, 6, 3, 8, 5, 4, 0, 3, 6, 5, 7, 7, 3, 9, 4, 1, 2, 6, 1, 4,\n",
      "        9, 0, 6, 2, 8, 5, 5, 0, 6, 5, 9, 0, 0, 7, 3, 3, 3, 2, 5, 5, 4, 3, 0, 6,\n",
      "        6, 9, 4, 5, 9, 8, 9, 3, 2, 8, 3, 4, 6, 5, 9, 6], device='cuda:0')\n",
      "        predict = tensor([3, 6, 8, 7, 5, 6, 3, 8, 5, 4, 0, 3, 6, 5, 7, 7, 3, 9, 4, 1, 2, 6, 1, 4,\n",
      "        9, 0, 6, 2, 8, 5, 5, 0, 6, 5, 9, 0, 0, 7, 3, 3, 3, 2, 5, 5, 4, 3, 0, 6,\n",
      "        6, 9, 4, 5, 7, 8, 9, 3, 2, 8, 3, 4, 6, 5, 9, 6], device='cuda:0')\n",
      "        accuracy = 0.96875\n",
      "i = 4,  loss = 0.02838878333568573,\n",
      "        labels = tensor([3, 6, 7, 3, 7, 7, 8, 4, 0, 1, 1, 2, 7, 1, 6, 4, 6, 8, 9, 0, 9, 6, 3, 5,\n",
      "        7, 1, 7, 4, 2, 4, 4, 2, 5, 4, 5, 3, 4, 4, 5, 5, 0, 4, 1, 2, 7, 0, 0, 6,\n",
      "        8, 5, 1, 0, 8, 9, 4, 5, 8, 5, 8, 9, 4, 7, 6, 3], device='cuda:0')\n",
      "        predict = tensor([3, 6, 7, 3, 7, 7, 8, 4, 0, 1, 1, 2, 7, 1, 6, 4, 6, 8, 9, 0, 9, 6, 3, 5,\n",
      "        7, 1, 7, 4, 2, 4, 4, 2, 5, 4, 5, 3, 4, 4, 5, 5, 0, 4, 1, 2, 7, 0, 0, 6,\n",
      "        8, 5, 1, 0, 8, 9, 4, 5, 8, 5, 8, 9, 4, 7, 6, 3], device='cuda:0')\n",
      "        accuracy = 1.0\n",
      "i = 6,  loss = 0.013471581041812897,\n",
      "        labels = tensor([6, 2, 6, 5, 9, 6, 4, 5, 5, 4, 5, 1, 1, 4, 9, 2, 8, 5, 0, 0, 9, 6, 5, 6,\n",
      "        7, 4, 9, 8, 7, 4, 2, 6, 1, 8, 3, 3, 5, 1, 2, 5, 7, 9, 1, 7, 9, 5, 4, 8,\n",
      "        5, 2, 7, 3, 6, 1, 2, 1, 7, 1, 8, 4, 8, 3, 9, 0], device='cuda:0')\n",
      "        predict = tensor([6, 2, 6, 5, 9, 6, 4, 5, 5, 4, 5, 1, 1, 4, 9, 2, 8, 5, 0, 0, 9, 6, 5, 6,\n",
      "        7, 4, 9, 8, 7, 4, 2, 6, 1, 8, 3, 3, 5, 1, 2, 5, 7, 9, 1, 7, 9, 5, 4, 8,\n",
      "        5, 2, 7, 3, 6, 1, 2, 1, 7, 1, 8, 4, 8, 3, 9, 0], device='cuda:0')\n",
      "        accuracy = 1.0\n",
      "i = 8,  loss = 0.021273298189044,\n",
      "        labels = tensor([1, 1, 2, 1, 7, 3, 4, 0, 6, 4, 9, 4, 7, 0, 0, 0, 5, 7, 6, 4, 9, 0, 7, 0,\n",
      "        0, 6, 9, 7, 9, 5, 5, 8, 5, 4, 1, 8, 4, 1, 1, 1, 1, 6, 5, 8, 6, 1, 0, 2,\n",
      "        8, 6, 6, 6, 9, 3, 5, 9, 9, 2, 7, 2, 1, 4, 6, 0], device='cuda:0')\n",
      "        predict = tensor([1, 1, 2, 1, 7, 3, 4, 0, 6, 4, 9, 4, 7, 0, 0, 0, 5, 7, 6, 4, 9, 0, 7, 0,\n",
      "        0, 6, 9, 7, 9, 5, 5, 8, 5, 4, 1, 8, 4, 1, 1, 1, 1, 6, 5, 8, 6, 1, 0, 2,\n",
      "        8, 6, 6, 6, 9, 3, 5, 9, 9, 2, 7, 2, 1, 4, 6, 0], device='cuda:0')\n",
      "        accuracy = 1.0\n",
      "i = 10,  loss = 0.016077280044555664,\n",
      "        labels = tensor([6, 2, 7, 0, 2, 1, 3, 9, 3, 1, 5, 0, 4, 2, 7, 5, 6, 5, 6, 3, 8, 6, 5, 3,\n",
      "        3, 7, 8, 9, 5, 2, 5, 1, 9, 0, 8, 4, 1, 2, 8, 6, 1, 5, 4, 5, 7, 6, 0, 1,\n",
      "        5, 4, 3, 5, 0, 9, 5, 6, 1, 8, 0, 2, 9, 0, 5, 9], device='cuda:0')\n",
      "        predict = tensor([6, 2, 7, 0, 2, 1, 3, 9, 3, 1, 5, 0, 4, 2, 7, 5, 6, 5, 6, 3, 8, 6, 5, 3,\n",
      "        3, 7, 8, 9, 5, 2, 5, 1, 9, 0, 8, 4, 1, 2, 8, 6, 1, 5, 4, 5, 7, 6, 0, 1,\n",
      "        5, 4, 3, 5, 0, 9, 5, 6, 1, 8, 0, 2, 9, 0, 5, 9], device='cuda:0')\n",
      "        accuracy = 1.0\n",
      "i = 12,  loss = 0.008417259901762009,\n",
      "        labels = tensor([5, 6, 2, 2, 6, 1, 5, 2, 5, 4, 3, 4, 3, 8, 6, 7, 3, 4, 1, 9, 6, 6, 8, 5,\n",
      "        0, 4, 9, 0, 5, 1, 8, 7, 0, 0, 4, 2, 9, 1, 5, 8, 0, 9, 8, 4, 4, 9, 2, 0,\n",
      "        2, 1, 1, 8, 4, 9, 1, 9, 8, 1, 2, 3, 9, 0, 6, 9], device='cuda:0')\n",
      "        predict = tensor([5, 6, 2, 2, 6, 1, 5, 2, 5, 4, 3, 4, 3, 8, 6, 7, 3, 4, 1, 9, 6, 6, 8, 5,\n",
      "        0, 4, 9, 0, 5, 1, 8, 7, 0, 0, 4, 2, 9, 1, 5, 8, 0, 9, 8, 4, 4, 9, 2, 0,\n",
      "        2, 1, 1, 8, 4, 9, 1, 9, 8, 1, 2, 3, 9, 0, 6, 9], device='cuda:0')\n",
      "        accuracy = 1.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5dcac4099948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimgNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/liux19-8ViQiPRs/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/liux19-8ViQiPRs/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/liux19-8ViQiPRs/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/liux19-8ViQiPRs/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e0e5180aee24>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrow_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-314d850788a1>\u001b[0m in \u001b[0;36mcrop_img\u001b[0;34m(folder, is_debug)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmask_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrgb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmask_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m460\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m540\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_img\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcenter_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindContourCenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# center_rgb = np.array([center_mask[0] + 20, center_mask[1] + 100])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(imgNet.parameters(), lr=0.001)\n",
    "\n",
    "imgNet.train()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "imgNet = imgNet.to(device)\n",
    "\n",
    "for i, (imgs, lbs) in enumerate(train_loader):\n",
    "    imgs = imgs.float().to(device)\n",
    "    lbs = lbs.to(device)\n",
    "    outputs = imgNet(imgs)\n",
    "    loss = loss_func(outputs, lbs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    predict = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "    # print(int(round(time.time() * 1000)))\n",
    "    if i % 2 == 0:\n",
    "        print(f\"\"\"i = {i},  loss = {loss},\n",
    "        labels = {lbs}\n",
    "        predict = {predict}\n",
    "        accuracy = {float(sum(lbs == predict))/float(lbs.size(0))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型， 请谨慎操作， 会覆盖文件中的模型\n",
    "torch.save(imgNet.state_dict(), './imgNet.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i = 0, \n",
      " lables = tensor([3, 9, 9, 4, 0, 4, 5, 7, 1, 2, 4, 9, 2, 4, 3, 0, 6, 6, 5, 3, 7, 6, 5, 1,\n",
      "        4, 3, 2, 7, 0, 8, 0, 8, 2, 7, 4, 8, 8, 1, 7, 3, 8, 9, 5, 3, 0, 6, 5, 0,\n",
      "        2, 2], device='cuda:0'), \n",
      " predict = tensor([3, 9, 9, 4, 0, 4, 5, 7, 1, 2, 4, 9, 2, 4, 3, 0, 6, 6, 5, 3, 7, 6, 5, 1,\n",
      "        4, 3, 2, 7, 0, 8, 0, 8, 2, 7, 4, 8, 8, 1, 7, 3, 8, 9, 5, 3, 0, 6, 5, 0,\n",
      "        2, 2], device='cuda:0')  \n",
      " accuracy = 1.0\n",
      "i = 1, \n",
      " lables = tensor([8, 5, 1, 5, 3, 0, 6, 8, 4, 7, 0, 8, 1, 6, 1, 2, 1, 5, 2, 0, 5, 1, 1, 2,\n",
      "        8, 0, 6, 7, 5, 4, 3, 3, 7, 5, 9, 2, 3, 0, 1, 5, 8, 8, 7, 0, 7, 6, 2, 2,\n",
      "        0, 5], device='cuda:0'), \n",
      " predict = tensor([8, 5, 1, 5, 3, 0, 6, 8, 4, 7, 0, 8, 1, 6, 1, 2, 1, 5, 2, 0, 5, 1, 1, 2,\n",
      "        8, 0, 6, 7, 5, 4, 3, 3, 7, 5, 9, 2, 3, 0, 1, 5, 8, 8, 7, 0, 7, 6, 2, 2,\n",
      "        0, 5], device='cuda:0')  \n",
      " accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "val_loader = data.DataLoader(ImageSet('val'), batch_size=50, shuffle=False)\n",
    "\n",
    "imgNet.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "imgNet = imgNet.to(device)\n",
    "\n",
    "for i, (imgs, lbs) in enumerate(val_loader):\n",
    "    imgs = imgs.float().to(device)\n",
    "    lbs = lbs.to(device)\n",
    "    outputs = imgNet(imgs)\n",
    "    predict = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "    if i % 1 == 0:\n",
    "        print(f\"i = {i}, \\n lables = {lbs}, \\n predict = {predict}  \\n accuracy = {float(sum(lbs == predict))/float(lbs.size(0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folders = glob.glob(f'./dataset/task2/test/*/*/')\n",
    "class TestSet(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.length = len(test_folders)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        folder = test_folders[index]\n",
    "        rgb_img = crop_img(folder, False)\n",
    "        img = np.zeros((3,200,200), dtype='uint8')\n",
    "        row_begin = 100 - int(rgb_img.shape[1]/2)\n",
    "        col_begin = 100 - int(rgb_img.shape[2]/2)\n",
    "        img[:, row_begin:row_begin+rgb_img.shape[1], col_begin:col_begin+rgb_img.shape[2]] = rgb_img\n",
    "        img = torch.from_numpy(img)\n",
    "        return img, folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "test_loader = data.DataLoader(TestSet(), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i = 0\n",
      "        predict = tensor([6, 7, 8, 8, 3, 5, 8, 1, 3, 6, 8, 0, 9, 9, 1, 1, 3, 4, 3, 6, 0, 6, 1, 7,\n",
      "        0, 9, 2, 7, 0, 2, 3, 6, 3, 8, 8, 2, 1, 2, 8, 9, 5, 2, 1, 5, 5, 3, 5, 8,\n",
      "        0, 1, 8, 1, 7, 2, 1, 3, 0, 1, 5, 8, 9, 6, 2, 2], device='cuda:0')\n",
      "i = 1\n",
      "        predict = tensor([6, 4, 6, 7, 2, 7, 7, 0, 5, 8, 1, 3, 4, 9, 2, 4, 9, 0, 2, 5, 3, 1, 6, 7,\n",
      "        6, 3, 4, 0, 3, 3, 9, 5, 6, 9, 2, 7, 1, 7, 9, 3, 1, 6, 9, 5, 8, 9, 5, 0,\n",
      "        0, 6, 5, 3, 4, 2, 9, 8, 7, 1, 1, 7, 3, 6, 7, 1], device='cuda:0')\n",
      "i = 2\n",
      "        predict = tensor([2, 8, 4, 5, 6, 7, 6, 6, 5, 9, 1, 8, 4, 8, 2, 0, 3, 7, 5, 8, 5, 6, 8, 7,\n",
      "        5, 1, 7, 8, 5, 5, 2, 7, 7, 6, 0, 9, 3, 5, 4, 5, 0, 7, 3, 6, 2, 6, 3, 0,\n",
      "        3, 9, 5, 3, 3, 9, 7, 2, 0, 0, 8, 2, 8, 4, 3, 0], device='cuda:0')\n",
      "i = 3\n",
      "        predict = tensor([0, 7, 3, 5, 6, 7, 3, 8, 7, 2, 1, 5, 7, 1, 9, 6, 2, 8, 6, 2, 4, 1, 8, 7,\n",
      "        1, 8, 5, 5, 2, 1, 7, 0, 5, 7, 5, 0, 7, 3, 8, 4, 9, 8, 3, 3, 7, 7, 2, 3,\n",
      "        8, 9, 8, 6, 2, 7, 2, 3, 8, 6, 9, 8, 6, 3, 0, 8], device='cuda:0')\n",
      "i = 4\n",
      "        predict = tensor([5, 1, 7, 4, 5, 1, 0, 7, 3, 3, 2, 5, 8, 3, 5, 1, 3, 7, 1, 3, 4, 8, 6, 6,\n",
      "        5, 3, 2, 7, 3, 6, 7, 3, 3, 5, 5, 2, 4, 3, 5, 6, 1, 8, 5, 0, 8, 8, 0, 8,\n",
      "        3, 3, 9, 7, 9, 2, 5, 0, 7, 8, 9, 8, 6, 6, 6, 7], device='cuda:0')\n",
      "i = 5\n",
      "        predict = tensor([7, 1, 1, 6, 6, 1, 3, 7, 8, 8, 9, 6, 3, 6, 4, 9, 5, 5, 4, 5, 8, 8, 7, 3,\n",
      "        7, 8, 4, 8, 6, 1, 5, 0, 8, 2, 9, 1, 3, 1, 3, 4, 9, 7, 9, 3, 5, 6, 9, 8,\n",
      "        8, 3, 4, 9, 1, 1, 8, 2, 5, 2, 1, 3, 8, 6, 3, 1], device='cuda:0')\n",
      "i = 6\n",
      "        predict = tensor([7, 6, 3, 8, 8, 6, 5, 5, 7, 3, 5, 2, 6, 4, 5, 2, 7, 6, 5, 5, 9, 5, 2, 5,\n",
      "        9, 1, 9, 1, 9, 2, 6, 8, 7, 7, 5, 8, 1, 5, 0, 5, 2, 7, 6, 1, 3, 7, 1, 7,\n",
      "        5, 3, 3, 3, 7, 4, 1, 5, 6, 3, 0, 6, 2, 9, 1, 3], device='cuda:0')\n",
      "i = 7\n",
      "        predict = tensor([4, 0, 3, 7, 8, 7, 1, 2, 2, 2, 5, 2, 1, 7, 2, 7, 3, 9, 4, 3, 3, 4, 3, 1,\n",
      "        8, 3, 5, 1, 7, 9, 7, 4, 6, 5, 1, 6, 0, 7, 4, 8, 3, 2, 7, 7, 2, 5, 2, 1,\n",
      "        6, 1, 6, 0], device='cuda:0')\n",
      "['./dataset/task2/test/6/video_0043/', './dataset/task2/test/6/video_0030/', './dataset/task2/test/6/video_0005/', './dataset/task2/test/6/video_0031/', './dataset/task2/test/6/video_0012/', './dataset/task2/test/6/video_0021/', './dataset/task2/test/6/video_0033/', './dataset/task2/test/6/video_0003/', './dataset/task2/test/6/video_0028/', './dataset/task2/test/6/video_0025/', './dataset/task2/test/6/video_0048/', './dataset/task2/test/6/video_0047/', './dataset/task2/test/6/video_0002/', './dataset/task2/test/6/video_0029/', './dataset/task2/test/6/video_0026/', './dataset/task2/test/6/video_0007/', './dataset/task2/test/6/video_0036/', './dataset/task2/test/6/video_0042/', './dataset/task2/test/6/video_0019/', './dataset/task2/test/6/video_0039/', './dataset/task2/test/6/video_0006/', './dataset/task2/test/6/video_0016/', './dataset/task2/test/6/video_0011/', './dataset/task2/test/6/video_0046/', './dataset/task2/test/6/video_0037/', './dataset/task2/test/6/video_0024/', './dataset/task2/test/6/video_0013/', './dataset/task2/test/6/video_0041/', './dataset/task2/test/6/video_0017/', './dataset/task2/test/6/video_0022/', './dataset/task2/test/6/video_0032/', './dataset/task2/test/6/video_0040/', './dataset/task2/test/6/video_0023/', './dataset/task2/test/6/video_0049/', './dataset/task2/test/6/video_0038/', './dataset/task2/test/6/video_0015/', './dataset/task2/test/6/video_0010/', './dataset/task2/test/6/video_0014/', './dataset/task2/test/6/video_0027/', './dataset/task2/test/6/video_0018/', './dataset/task2/test/6/video_0045/', './dataset/task2/test/6/video_0008/', './dataset/task2/test/6/video_0001/', './dataset/task2/test/6/video_0035/', './dataset/task2/test/6/video_0004/', './dataset/task2/test/6/video_0034/', './dataset/task2/test/6/video_0020/', './dataset/task2/test/6/video_0009/', './dataset/task2/test/6/video_0044/', './dataset/task2/test/6/video_0000/', './dataset/task2/test/8/video_0043/', './dataset/task2/test/8/video_0030/', './dataset/task2/test/8/video_0005/', './dataset/task2/test/8/video_0031/', './dataset/task2/test/8/video_0012/', './dataset/task2/test/8/video_0021/', './dataset/task2/test/8/video_0033/', './dataset/task2/test/8/video_0003/', './dataset/task2/test/8/video_0028/', './dataset/task2/test/8/video_0025/', './dataset/task2/test/8/video_0048/', './dataset/task2/test/8/video_0047/', './dataset/task2/test/8/video_0002/', './dataset/task2/test/8/video_0029/', './dataset/task2/test/8/video_0026/', './dataset/task2/test/8/video_0007/', './dataset/task2/test/8/video_0036/', './dataset/task2/test/8/video_0042/', './dataset/task2/test/8/video_0019/', './dataset/task2/test/8/video_0039/', './dataset/task2/test/8/video_0006/', './dataset/task2/test/8/video_0016/', './dataset/task2/test/8/video_0011/', './dataset/task2/test/8/video_0046/', './dataset/task2/test/8/video_0037/', './dataset/task2/test/8/video_0024/', './dataset/task2/test/8/video_0013/', './dataset/task2/test/8/video_0041/', './dataset/task2/test/8/video_0017/', './dataset/task2/test/8/video_0022/', './dataset/task2/test/8/video_0032/', './dataset/task2/test/8/video_0040/', './dataset/task2/test/8/video_0023/', './dataset/task2/test/8/video_0049/', './dataset/task2/test/8/video_0038/', './dataset/task2/test/8/video_0015/', './dataset/task2/test/8/video_0010/', './dataset/task2/test/8/video_0014/', './dataset/task2/test/8/video_0027/', './dataset/task2/test/8/video_0018/', './dataset/task2/test/8/video_0045/', './dataset/task2/test/8/video_0008/', './dataset/task2/test/8/video_0001/', './dataset/task2/test/8/video_0035/', './dataset/task2/test/8/video_0004/', './dataset/task2/test/8/video_0034/', './dataset/task2/test/8/video_0020/', './dataset/task2/test/8/video_0009/', './dataset/task2/test/8/video_0044/', './dataset/task2/test/8/video_0000/', './dataset/task2/test/4/video_0043/', './dataset/task2/test/4/video_0030/', './dataset/task2/test/4/video_0005/', './dataset/task2/test/4/video_0031/', './dataset/task2/test/4/video_0012/', './dataset/task2/test/4/video_0021/', './dataset/task2/test/4/video_0033/', './dataset/task2/test/4/video_0003/', './dataset/task2/test/4/video_0028/', './dataset/task2/test/4/video_0025/', './dataset/task2/test/4/video_0048/', './dataset/task2/test/4/video_0047/', './dataset/task2/test/4/video_0002/', './dataset/task2/test/4/video_0029/', './dataset/task2/test/4/video_0026/', './dataset/task2/test/4/video_0007/', './dataset/task2/test/4/video_0036/', './dataset/task2/test/4/video_0042/', './dataset/task2/test/4/video_0019/', './dataset/task2/test/4/video_0039/', './dataset/task2/test/4/video_0006/', './dataset/task2/test/4/video_0016/', './dataset/task2/test/4/video_0011/', './dataset/task2/test/4/video_0046/', './dataset/task2/test/4/video_0037/', './dataset/task2/test/4/video_0024/', './dataset/task2/test/4/video_0013/', './dataset/task2/test/4/video_0041/', './dataset/task2/test/4/video_0017/', './dataset/task2/test/4/video_0022/', './dataset/task2/test/4/video_0032/', './dataset/task2/test/4/video_0040/', './dataset/task2/test/4/video_0023/', './dataset/task2/test/4/video_0049/', './dataset/task2/test/4/video_0038/', './dataset/task2/test/4/video_0015/', './dataset/task2/test/4/video_0010/', './dataset/task2/test/4/video_0014/', './dataset/task2/test/4/video_0027/', './dataset/task2/test/4/video_0018/', './dataset/task2/test/4/video_0045/', './dataset/task2/test/4/video_0008/', './dataset/task2/test/4/video_0001/', './dataset/task2/test/4/video_0035/', './dataset/task2/test/4/video_0004/', './dataset/task2/test/4/video_0034/', './dataset/task2/test/4/video_0020/', './dataset/task2/test/4/video_0009/', './dataset/task2/test/4/video_0044/', './dataset/task2/test/4/video_0000/', './dataset/task2/test/2/video_0043/', './dataset/task2/test/2/video_0030/', './dataset/task2/test/2/video_0005/', './dataset/task2/test/2/video_0031/', './dataset/task2/test/2/video_0012/', './dataset/task2/test/2/video_0021/', './dataset/task2/test/2/video_0033/', './dataset/task2/test/2/video_0003/', './dataset/task2/test/2/video_0028/', './dataset/task2/test/2/video_0025/', './dataset/task2/test/2/video_0048/', './dataset/task2/test/2/video_0047/', './dataset/task2/test/2/video_0002/', './dataset/task2/test/2/video_0029/', './dataset/task2/test/2/video_0026/', './dataset/task2/test/2/video_0007/', './dataset/task2/test/2/video_0036/', './dataset/task2/test/2/video_0042/', './dataset/task2/test/2/video_0019/', './dataset/task2/test/2/video_0039/', './dataset/task2/test/2/video_0006/', './dataset/task2/test/2/video_0016/', './dataset/task2/test/2/video_0011/', './dataset/task2/test/2/video_0046/', './dataset/task2/test/2/video_0037/', './dataset/task2/test/2/video_0024/', './dataset/task2/test/2/video_0013/', './dataset/task2/test/2/video_0041/', './dataset/task2/test/2/video_0017/', './dataset/task2/test/2/video_0022/', './dataset/task2/test/2/video_0032/', './dataset/task2/test/2/video_0040/', './dataset/task2/test/2/video_0023/', './dataset/task2/test/2/video_0049/', './dataset/task2/test/2/video_0038/', './dataset/task2/test/2/video_0015/', './dataset/task2/test/2/video_0010/', './dataset/task2/test/2/video_0014/', './dataset/task2/test/2/video_0027/', './dataset/task2/test/2/video_0018/', './dataset/task2/test/2/video_0045/', './dataset/task2/test/2/video_0008/', './dataset/task2/test/2/video_0001/', './dataset/task2/test/2/video_0035/', './dataset/task2/test/2/video_0004/', './dataset/task2/test/2/video_0034/', './dataset/task2/test/2/video_0020/', './dataset/task2/test/2/video_0009/', './dataset/task2/test/2/video_0044/', './dataset/task2/test/2/video_0000/', './dataset/task2/test/3/video_0043/', './dataset/task2/test/3/video_0030/', './dataset/task2/test/3/video_0005/', './dataset/task2/test/3/video_0031/', './dataset/task2/test/3/video_0012/', './dataset/task2/test/3/video_0021/', './dataset/task2/test/3/video_0033/', './dataset/task2/test/3/video_0003/', './dataset/task2/test/3/video_0028/', './dataset/task2/test/3/video_0025/', './dataset/task2/test/3/video_0048/', './dataset/task2/test/3/video_0047/', './dataset/task2/test/3/video_0002/', './dataset/task2/test/3/video_0029/', './dataset/task2/test/3/video_0026/', './dataset/task2/test/3/video_0007/', './dataset/task2/test/3/video_0036/', './dataset/task2/test/3/video_0042/', './dataset/task2/test/3/video_0019/', './dataset/task2/test/3/video_0039/', './dataset/task2/test/3/video_0006/', './dataset/task2/test/3/video_0016/', './dataset/task2/test/3/video_0011/', './dataset/task2/test/3/video_0046/', './dataset/task2/test/3/video_0037/', './dataset/task2/test/3/video_0024/', './dataset/task2/test/3/video_0013/', './dataset/task2/test/3/video_0041/', './dataset/task2/test/3/video_0017/', './dataset/task2/test/3/video_0022/', './dataset/task2/test/3/video_0032/', './dataset/task2/test/3/video_0040/', './dataset/task2/test/3/video_0023/', './dataset/task2/test/3/video_0049/', './dataset/task2/test/3/video_0038/', './dataset/task2/test/3/video_0015/', './dataset/task2/test/3/video_0010/', './dataset/task2/test/3/video_0014/', './dataset/task2/test/3/video_0027/', './dataset/task2/test/3/video_0018/', './dataset/task2/test/3/video_0045/', './dataset/task2/test/3/video_0008/', './dataset/task2/test/3/video_0001/', './dataset/task2/test/3/video_0035/', './dataset/task2/test/3/video_0004/', './dataset/task2/test/3/video_0034/', './dataset/task2/test/3/video_0020/', './dataset/task2/test/3/video_0009/', './dataset/task2/test/3/video_0044/', './dataset/task2/test/3/video_0000/', './dataset/task2/test/5/video_0043/', './dataset/task2/test/5/video_0030/', './dataset/task2/test/5/video_0005/', './dataset/task2/test/5/video_0031/', './dataset/task2/test/5/video_0012/', './dataset/task2/test/5/video_0021/', './dataset/task2/test/5/video_0033/', './dataset/task2/test/5/video_0003/', './dataset/task2/test/5/video_0028/', './dataset/task2/test/5/video_0025/', './dataset/task2/test/5/video_0048/', './dataset/task2/test/5/video_0047/', './dataset/task2/test/5/video_0002/', './dataset/task2/test/5/video_0029/', './dataset/task2/test/5/video_0026/', './dataset/task2/test/5/video_0007/', './dataset/task2/test/5/video_0036/', './dataset/task2/test/5/video_0042/', './dataset/task2/test/5/video_0019/', './dataset/task2/test/5/video_0039/', './dataset/task2/test/5/video_0006/', './dataset/task2/test/5/video_0016/', './dataset/task2/test/5/video_0011/', './dataset/task2/test/5/video_0046/', './dataset/task2/test/5/video_0037/', './dataset/task2/test/5/video_0024/', './dataset/task2/test/5/video_0013/', './dataset/task2/test/5/video_0041/', './dataset/task2/test/5/video_0017/', './dataset/task2/test/5/video_0022/', './dataset/task2/test/5/video_0032/', './dataset/task2/test/5/video_0040/', './dataset/task2/test/5/video_0023/', './dataset/task2/test/5/video_0049/', './dataset/task2/test/5/video_0038/', './dataset/task2/test/5/video_0015/', './dataset/task2/test/5/video_0010/', './dataset/task2/test/5/video_0014/', './dataset/task2/test/5/video_0027/', './dataset/task2/test/5/video_0018/', './dataset/task2/test/5/video_0045/', './dataset/task2/test/5/video_0008/', './dataset/task2/test/5/video_0001/', './dataset/task2/test/5/video_0035/', './dataset/task2/test/5/video_0004/', './dataset/task2/test/5/video_0034/', './dataset/task2/test/5/video_0020/', './dataset/task2/test/5/video_0009/', './dataset/task2/test/5/video_0044/', './dataset/task2/test/5/video_0000/', './dataset/task2/test/9/video_0043/', './dataset/task2/test/9/video_0030/', './dataset/task2/test/9/video_0005/', './dataset/task2/test/9/video_0031/', './dataset/task2/test/9/video_0012/', './dataset/task2/test/9/video_0021/', './dataset/task2/test/9/video_0033/', './dataset/task2/test/9/video_0003/', './dataset/task2/test/9/video_0028/', './dataset/task2/test/9/video_0025/', './dataset/task2/test/9/video_0048/', './dataset/task2/test/9/video_0047/', './dataset/task2/test/9/video_0002/', './dataset/task2/test/9/video_0029/', './dataset/task2/test/9/video_0026/', './dataset/task2/test/9/video_0007/', './dataset/task2/test/9/video_0036/', './dataset/task2/test/9/video_0042/', './dataset/task2/test/9/video_0019/', './dataset/task2/test/9/video_0039/', './dataset/task2/test/9/video_0006/', './dataset/task2/test/9/video_0016/', './dataset/task2/test/9/video_0011/', './dataset/task2/test/9/video_0046/', './dataset/task2/test/9/video_0037/', './dataset/task2/test/9/video_0024/', './dataset/task2/test/9/video_0013/', './dataset/task2/test/9/video_0041/', './dataset/task2/test/9/video_0017/', './dataset/task2/test/9/video_0022/', './dataset/task2/test/9/video_0032/', './dataset/task2/test/9/video_0040/', './dataset/task2/test/9/video_0023/', './dataset/task2/test/9/video_0049/', './dataset/task2/test/9/video_0038/', './dataset/task2/test/9/video_0015/', './dataset/task2/test/9/video_0010/', './dataset/task2/test/9/video_0014/', './dataset/task2/test/9/video_0027/', './dataset/task2/test/9/video_0018/', './dataset/task2/test/9/video_0045/', './dataset/task2/test/9/video_0008/', './dataset/task2/test/9/video_0001/', './dataset/task2/test/9/video_0035/', './dataset/task2/test/9/video_0004/', './dataset/task2/test/9/video_0034/', './dataset/task2/test/9/video_0020/', './dataset/task2/test/9/video_0009/', './dataset/task2/test/9/video_0044/', './dataset/task2/test/9/video_0000/', './dataset/task2/test/1/video_0043/', './dataset/task2/test/1/video_0030/', './dataset/task2/test/1/video_0005/', './dataset/task2/test/1/video_0031/', './dataset/task2/test/1/video_0012/', './dataset/task2/test/1/video_0021/', './dataset/task2/test/1/video_0033/', './dataset/task2/test/1/video_0003/', './dataset/task2/test/1/video_0028/', './dataset/task2/test/1/video_0025/', './dataset/task2/test/1/video_0048/', './dataset/task2/test/1/video_0047/', './dataset/task2/test/1/video_0002/', './dataset/task2/test/1/video_0029/', './dataset/task2/test/1/video_0026/', './dataset/task2/test/1/video_0007/', './dataset/task2/test/1/video_0036/', './dataset/task2/test/1/video_0042/', './dataset/task2/test/1/video_0019/', './dataset/task2/test/1/video_0039/', './dataset/task2/test/1/video_0006/', './dataset/task2/test/1/video_0016/', './dataset/task2/test/1/video_0011/', './dataset/task2/test/1/video_0046/', './dataset/task2/test/1/video_0037/', './dataset/task2/test/1/video_0024/', './dataset/task2/test/1/video_0013/', './dataset/task2/test/1/video_0041/', './dataset/task2/test/1/video_0017/', './dataset/task2/test/1/video_0022/', './dataset/task2/test/1/video_0032/', './dataset/task2/test/1/video_0040/', './dataset/task2/test/1/video_0023/', './dataset/task2/test/1/video_0049/', './dataset/task2/test/1/video_0038/', './dataset/task2/test/1/video_0015/', './dataset/task2/test/1/video_0010/', './dataset/task2/test/1/video_0014/', './dataset/task2/test/1/video_0027/', './dataset/task2/test/1/video_0018/', './dataset/task2/test/1/video_0045/', './dataset/task2/test/1/video_0008/', './dataset/task2/test/1/video_0001/', './dataset/task2/test/1/video_0035/', './dataset/task2/test/1/video_0004/', './dataset/task2/test/1/video_0034/', './dataset/task2/test/1/video_0020/', './dataset/task2/test/1/video_0009/', './dataset/task2/test/1/video_0044/', './dataset/task2/test/1/video_0000/', './dataset/task2/test/7/video_0043/', './dataset/task2/test/7/video_0030/', './dataset/task2/test/7/video_0005/', './dataset/task2/test/7/video_0031/', './dataset/task2/test/7/video_0012/', './dataset/task2/test/7/video_0021/', './dataset/task2/test/7/video_0033/', './dataset/task2/test/7/video_0003/', './dataset/task2/test/7/video_0028/', './dataset/task2/test/7/video_0025/', './dataset/task2/test/7/video_0048/', './dataset/task2/test/7/video_0047/', './dataset/task2/test/7/video_0002/', './dataset/task2/test/7/video_0029/', './dataset/task2/test/7/video_0026/', './dataset/task2/test/7/video_0007/', './dataset/task2/test/7/video_0036/', './dataset/task2/test/7/video_0042/', './dataset/task2/test/7/video_0019/', './dataset/task2/test/7/video_0039/', './dataset/task2/test/7/video_0006/', './dataset/task2/test/7/video_0016/', './dataset/task2/test/7/video_0011/', './dataset/task2/test/7/video_0046/', './dataset/task2/test/7/video_0037/', './dataset/task2/test/7/video_0024/', './dataset/task2/test/7/video_0013/', './dataset/task2/test/7/video_0041/', './dataset/task2/test/7/video_0017/', './dataset/task2/test/7/video_0022/', './dataset/task2/test/7/video_0032/', './dataset/task2/test/7/video_0040/', './dataset/task2/test/7/video_0023/', './dataset/task2/test/7/video_0049/', './dataset/task2/test/7/video_0038/', './dataset/task2/test/7/video_0015/', './dataset/task2/test/7/video_0010/', './dataset/task2/test/7/video_0014/', './dataset/task2/test/7/video_0027/', './dataset/task2/test/7/video_0018/', './dataset/task2/test/7/video_0045/', './dataset/task2/test/7/video_0008/', './dataset/task2/test/7/video_0001/', './dataset/task2/test/7/video_0035/', './dataset/task2/test/7/video_0004/', './dataset/task2/test/7/video_0034/', './dataset/task2/test/7/video_0020/', './dataset/task2/test/7/video_0009/', './dataset/task2/test/7/video_0044/', './dataset/task2/test/7/video_0000/', './dataset/task2/test/0/video_0043/', './dataset/task2/test/0/video_0030/', './dataset/task2/test/0/video_0005/', './dataset/task2/test/0/video_0031/', './dataset/task2/test/0/video_0012/', './dataset/task2/test/0/video_0021/', './dataset/task2/test/0/video_0033/', './dataset/task2/test/0/video_0003/', './dataset/task2/test/0/video_0028/', './dataset/task2/test/0/video_0025/', './dataset/task2/test/0/video_0048/', './dataset/task2/test/0/video_0047/', './dataset/task2/test/0/video_0002/', './dataset/task2/test/0/video_0029/', './dataset/task2/test/0/video_0026/', './dataset/task2/test/0/video_0007/', './dataset/task2/test/0/video_0036/', './dataset/task2/test/0/video_0042/', './dataset/task2/test/0/video_0019/', './dataset/task2/test/0/video_0039/', './dataset/task2/test/0/video_0006/', './dataset/task2/test/0/video_0016/', './dataset/task2/test/0/video_0011/', './dataset/task2/test/0/video_0046/', './dataset/task2/test/0/video_0037/', './dataset/task2/test/0/video_0024/', './dataset/task2/test/0/video_0013/', './dataset/task2/test/0/video_0041/', './dataset/task2/test/0/video_0017/', './dataset/task2/test/0/video_0022/', './dataset/task2/test/0/video_0032/', './dataset/task2/test/0/video_0040/', './dataset/task2/test/0/video_0023/', './dataset/task2/test/0/video_0049/', './dataset/task2/test/0/video_0038/', './dataset/task2/test/0/video_0015/', './dataset/task2/test/0/video_0010/', './dataset/task2/test/0/video_0014/', './dataset/task2/test/0/video_0027/', './dataset/task2/test/0/video_0018/', './dataset/task2/test/0/video_0045/', './dataset/task2/test/0/video_0008/', './dataset/task2/test/0/video_0001/', './dataset/task2/test/0/video_0035/', './dataset/task2/test/0/video_0004/', './dataset/task2/test/0/video_0034/', './dataset/task2/test/0/video_0020/', './dataset/task2/test/0/video_0009/', './dataset/task2/test/0/video_0044/', './dataset/task2/test/0/video_0000/']\n",
      "[6, 7, 8, 8, 3, 5, 8, 1, 3, 6, 8, 0, 9, 9, 1, 1, 3, 4, 3, 6, 0, 6, 1, 7, 0, 9, 2, 7, 0, 2, 3, 6, 3, 8, 8, 2, 1, 2, 8, 9, 5, 2, 1, 5, 5, 3, 5, 8, 0, 1, 8, 1, 7, 2, 1, 3, 0, 1, 5, 8, 9, 6, 2, 2, 6, 4, 6, 7, 2, 7, 7, 0, 5, 8, 1, 3, 4, 9, 2, 4, 9, 0, 2, 5, 3, 1, 6, 7, 6, 3, 4, 0, 3, 3, 9, 5, 6, 9, 2, 7, 1, 7, 9, 3, 1, 6, 9, 5, 8, 9, 5, 0, 0, 6, 5, 3, 4, 2, 9, 8, 7, 1, 1, 7, 3, 6, 7, 1, 2, 8, 4, 5, 6, 7, 6, 6, 5, 9, 1, 8, 4, 8, 2, 0, 3, 7, 5, 8, 5, 6, 8, 7, 5, 1, 7, 8, 5, 5, 2, 7, 7, 6, 0, 9, 3, 5, 4, 5, 0, 7, 3, 6, 2, 6, 3, 0, 3, 9, 5, 3, 3, 9, 7, 2, 0, 0, 8, 2, 8, 4, 3, 0, 0, 7, 3, 5, 6, 7, 3, 8, 7, 2, 1, 5, 7, 1, 9, 6, 2, 8, 6, 2, 4, 1, 8, 7, 1, 8, 5, 5, 2, 1, 7, 0, 5, 7, 5, 0, 7, 3, 8, 4, 9, 8, 3, 3, 7, 7, 2, 3, 8, 9, 8, 6, 2, 7, 2, 3, 8, 6, 9, 8, 6, 3, 0, 8, 5, 1, 7, 4, 5, 1, 0, 7, 3, 3, 2, 5, 8, 3, 5, 1, 3, 7, 1, 3, 4, 8, 6, 6, 5, 3, 2, 7, 3, 6, 7, 3, 3, 5, 5, 2, 4, 3, 5, 6, 1, 8, 5, 0, 8, 8, 0, 8, 3, 3, 9, 7, 9, 2, 5, 0, 7, 8, 9, 8, 6, 6, 6, 7, 7, 1, 1, 6, 6, 1, 3, 7, 8, 8, 9, 6, 3, 6, 4, 9, 5, 5, 4, 5, 8, 8, 7, 3, 7, 8, 4, 8, 6, 1, 5, 0, 8, 2, 9, 1, 3, 1, 3, 4, 9, 7, 9, 3, 5, 6, 9, 8, 8, 3, 4, 9, 1, 1, 8, 2, 5, 2, 1, 3, 8, 6, 3, 1, 7, 6, 3, 8, 8, 6, 5, 5, 7, 3, 5, 2, 6, 4, 5, 2, 7, 6, 5, 5, 9, 5, 2, 5, 9, 1, 9, 1, 9, 2, 6, 8, 7, 7, 5, 8, 1, 5, 0, 5, 2, 7, 6, 1, 3, 7, 1, 7, 5, 3, 3, 3, 7, 4, 1, 5, 6, 3, 0, 6, 2, 9, 1, 3, 4, 0, 3, 7, 8, 7, 1, 2, 2, 2, 5, 2, 1, 7, 2, 7, 3, 9, 4, 3, 3, 4, 3, 1, 8, 3, 5, 1, 7, 9, 7, 4, 6, 5, 1, 6, 0, 7, 4, 8, 3, 2, 7, 7, 2, 5, 2, 1, 6, 1, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "imgNet.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "imgNet = imgNet.to(device)\n",
    "predict_all = []\n",
    "files_all = []\n",
    "for i, (imgs, folder) in enumerate(test_loader):\n",
    "    imgs = imgs.float().to(device)\n",
    "    outputs = imgNet(imgs)\n",
    "    predict = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "    # print(int(round(time.time() * 1000)))\n",
    "    if i % 1 == 0:\n",
    "        print(f\"\"\"i = {i}\n",
    "        predict = {predict}\"\"\")\n",
    "    predict_all += predict.tolist()\n",
    "    files_all += list(folder)\n",
    "print(files_all)\n",
    "print(predict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}